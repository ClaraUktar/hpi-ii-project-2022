# hpi-ii-project-2022

## Prerequisites

- Install [Docker](https://docs.docker.com/get-docker/) and [docker-compose](https://docs.docker.com/compose/install/)
- Install [Protobuf compiler (protoc)](https://grpc.io/docs/protoc-installation/)
- Install [jq](https://stedolan.github.io/jq/download/)

## Architecture

![](architecture.png)

### RB Website

The [Registerbekanntmachung website](https://www.handelsregisterbekanntmachungen.de/index.php?aktion=suche) contains
announcements concerning entries made into the companies, cooperatives and
partnerships registers within the electronic information and communication system. You can search the announcements.
Each announcement can be requested through the link below. You only need to pass the query parameters `rb_id`
and `land_abk`. For instance, we chose the state Rheinland-Pfalz `rp` with an announcement id of `56267`, which is the
new entry of the company BioNTech.

```shell
export STATE="rp" 
export RB_ID="56267"
curl -X GET  "https://www.handelsregisterbekanntmachungen.de/skripte/hrb.php?rb_id=$RB_ID&land_abk=$STATE"
```

### RB Crawler

The Registerbekanntmachung crawler (rb_crawler) sends a get request to the link above with parameters (`rb_id`
and `land_abk`)
passed to it and extracts the information from the response. The crawler uses the generated code from
the [protobuf schema](./proto/bakdata/corporate/v1/corporate.proto).
It creates a `Corporate` object with the fields defined in the schema. The crawler fills the object fields with the
extracted data from the website.
It then serializes the `Corporate` object to bytes so that Kafka can read it and produces it to the `corporate-events`
topic. After that it increments
the `rb_id` value and sends another GET request. This process continues until the end of the announcements is reached
,and the crawler will stop automatically.

### corporate-events topic

The `corporate-events` holds all the events (announcements) produced by the `rb_crawler`. Each message in a Kafka topic
consist of a key and value.

The key of this topic is of type `String`. The key is generated by the `rb_crawler`. The key
is a combination of the `land_abk` and the `rb_id`. If we consider the `rb_id` and `land_abk` from the example above the
key would look like this: `rp_56267`.

The value of the message contains more information like `event_name`, `event_date`, and more. Therefore, the value type
is complex type and needs a schema definition. We use [Protocol buffers](https://developers.google.com/protocol-buffers)
to define our schema. You can use the [Protobuf compiler (protoc)](https://grpc.io/docs/protoc-installation/) to
generate source code. You can use this generated source code to easily write and read your structured data to and
from a variety of data streams and using a variety of languages. The protobuf schema for the values is shown below:

```protobuf
syntax = "proto3";

package bakdata.corporate.v1;

message Corporate {
  string id = 1;
  uint32 rb_id = 2;
  string state = 3;
  string reference_id = 4;
  string event_date = 5;
  string event_type = 6;
  Status status = 7;
  string information = 8;
}

enum Status {
  STATUS_UNSPECIFIED = 0;
  STATUS_INACTIVE = 1;
  STATUS_ACTIVE = 2;
}
```

### Kafka-Connect

[Kafka Connect](https://docs.confluent.io/platform/current/connect/index.html) is tool to move large date sets into (
source) and out (sink) of Kafka. Here we only use the Sink connector, that consumes data from a kafka topic into a
secondary indexes such as Elasticsearch.

We use the [Elasticsearch Sink Connector](https://docs.confluent.io/kafka-connect-elasticsearch/current/overview.html)
to move the data from the `coporate-events` topic into the Elasticsearch.

## Setup

This project uses [poetry](https://python-poetry.org/) as a build tool.
To install all the dependencies, just run `poetry install`.

This project uses Protobuf for serializing and deserializing objects. We provided a
simple [protobuf schema](./proto/bakdata/corporate/v1/corporate.proto).
Furthermore, you need to generate the Python code from the proto file.
To do so run the [`generate-proto.sh`](./generate-proto.sh)
script.
This script uses the Protobuf compiler to generate the model class under the `build/gen/bakdata/corporate/v1` folder
with the name `corporate_pb2.py`.

## Run

### Infrastructure

Use `docker-compose up -d` to start all the services: [Zookeeper](https://zookeeper.apache.org/)
, [Kafka](https://kafka.apache.org/), [Schema
Registry](https://docs.confluent.io/platform/current/schema-registry/index.html)
, [Kafka REST Proxy]((https://github.com/confluentinc/kafka-rest)), [Kowl](https://github.com/redpanda-data/kowl),
[Kafka Connect](https://docs.confluent.io/platform/current/connect/index.html),
and [Elasticsearch](https://www.elastic.co/elasticsearch/).
After all the services are up and running you can use the [`create-topic.sh`](./rb_crawler/scripts/create-topic.sh)
script to create the `corporate-events` topic.
This script uses the Kafka REST Proxy to communicate with Kafka.

### Kafka Connect

After all the services are up and running, you need to configure Kafka Connect to use the Elasticsearch sink connector.
The config file is a JSON formatted file. We provided a [basic configuration file](./connect/elastic-sink.json).
You can find more information about the configuration properties on
the [official documentation page](https://docs.confluent.io/kafka-connect-elasticsearch/current/overview.html).

### RB Crawler
You can start the crawler with the command below:

```shell
python rb_cralwer/main.py --id $RB_ID --state $STATE
```
The `--id` option is an integer, which determines the initial event in the handelsregisterbekanntmachungen to be
crawled.

The `--state` option takes a string (only the ones listed above). This string defines the state where the crawler should
start from.

You can use the `--help` option to see the usage:
```
Usage: main.py [OPTIONS]

Options:
  -i, --id INTEGER                The rb_id to initialize the crawl from
  -s, --state [bw|by|be|br|hb|hh|he|mv|ni|nw|rp|sl|sn|st|sh|th]
                                  The state ISO code
  --help                          Show this message and exit.
```

## Query data

### Kowl

[Kowl](https://github.com/redpanda-data/kowl) is a web application that helps you manage and debug your Kafka/Redpanda
workloads effortlessly. You can create, update, and delete Kafka resources like: Topics, Kafka Connect configs.
You can see Kowl's dashboard in your browser under http://localhost:8080.

### Elasticsearch

To query the data from Elasticsearch you can use
the [query DSL](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl.html) of elastic. For example:

```shell
curl -X GET "localhost:9200/_search?pretty" -H 'Content-Type: application/json' -d'
{
    "query": {
        "match": {
            <field>
        }
    }
}
'
```

`<field>` is the field you wish to search. For example:

```
"reference_id":"HRB 41865"
```

